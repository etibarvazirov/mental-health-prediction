{
    "bert_dim": 768,
    "mlp_dim": 128,
    "numeric_dim": 12,
    "fusion_input_dim": 908,
    "hidden_layers": [
        256,
        128
    ],
    "output_dim": 1,
    "dropout": 0.2,
    "activation": "ReLU"
}
